import streamlit as st
import openai_api
from audiorecorder import audiorecorder # pip install streamlit-audiorecorder
from streamlit_chat import message as msg # pip install streamlit-chat


# pip install streamlit, streamlit-audiorecorder streamlit-chat
# ffmpeg(ì¸ì½”ë”©-ë””ì½”ë”© ë‹´ë‹¹) ë‹¤ìš´ë¡œë“œ https://www.gyan.dev/ffmpeg/builds/

def main():
    # ìƒë‹¨ ì œëª©
    st.set_page_config(
        page_title="ğŸ™ï¸Voice ChatbotğŸ™ï¸",
        page_icon='â­',
        layout = 'wide'
    )
    st.header('ğŸ¤Voice ChatbotğŸ¤')
    st.markdown('---')

    # ì±—ë´‡ ì‚¬ìš©ë²•
    with st.expander('ğŸ”µVoice Chatbot í”„ë¡œê·¸ë¨ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ğŸ”µ', expanded=True):
        st.write(
            """
            1. ë…¹ìŒí•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ ì§ˆë¬¸ì„ ë…¹ìŒí•©ë‹ˆë‹¤.
            2. ë…¹ìŒì´ ì™„ë£Œë˜ë©´ ìë™ìœ¼ë¡œ Whisper ëª¨ë¸ì´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ LLMì— ì§ˆì˜í•©ë‹ˆë‹¤.
            3. LLMì˜ ì‘ë‹µì„ ë‹¤ì‹œ TTS ëª¨ë¸ì„ ì‚¬ìš©í•´ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤.
            4. LLMì€ OpenAIì‚¬ì˜ GPT ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            5. ëª¨ë“  ì§ˆë¬¸/ë‹µë³€ì€ í…ìŠ¤íŠ¸ë¡œë„ ì œê³µí•©ë‹ˆë‹¤.
            """
        )
    st.markdown(' ') # í•œì¤„ ë„ê¸°

    system_instruction = 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤.'

    # session state ì´ˆê¸°í™”
    # - messages: LLM ì§ˆì˜ë¥¼ ìœ„í•œ ëŒ€í™” ë‚´ì—­/ì›¹í˜ì´ì§€ ì‹œê°í™”ìš© ëŒ€í™”ë‚´ì—­
    # - check_reset: ì´ˆê¸°í™”ë¥¼ ìœ„í•œ í”Œë ˆê·¸
    if 'chats' not in st.session_state:
        st.session_state['chats'] = [] # ì„ ì–¸

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [
            {'role': 'system', 'content': system_instruction}
        ]

    if 'check_reset' not in st.session_state:
        st.session_state['check_reset'] = False

    # ì‚¬ì´ë“œ ë°” - ëª¨ë¸ ì„ íƒ
    with st.sidebar:
        model = st.radio(label='GPT ëª¨ë¸ ì„ íƒ', options=['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o'])
        print('ëª¨ë¸: ', model)

        if st.button(label='ì´ˆê¸°í™”'):
            st.session_state['messages'] = [
                {'role': 'system', 'content': system_instruction}
            ]
            st.session_state['check_reset'] = True # í™”ë©´ ì •ë¦¬(reset)

    # ë…¹ìŒ / ì§ˆë¬¸ ë‹µë³€ ì¹¸
    col1, col2 = st.columns(2)
    with col1:
        st.subheader('ë…¹ìŒí•˜ëŸ¬ ê°€ê¸°ğŸ’¨')

        audio = audiorecorder()

        if (audio.duration_seconds > 0) and (st.session_state['check_reset']==False):
            # í™”ë©´ ìƒ ì¬ìƒ ê¸°ëŠ¥
            st.audio(audio.export().read()) # mp3êº¼ë‚´ì„œ ì „ë‹¬ (ì¬ìƒí•  ìˆ˜ ìˆëŠ” ëª¨ì–‘)
            # ì‚¬ìš©ì ìŒì„± >> í…ìŠ¤íŠ¸ ì¶”ì¶œ
            query = openai_api.stt(audio)  # speech->text
            print('Q: ', query)
            # LLM ì§ˆì˜
            st.session_state['messages'].append({'role': 'user', 'content': query}) # ì¶”ê°€
            response = openai_api.ask_gpt(st.session_state['messages'], model)
            print('A: ', response)
            st.session_state['messages'].append({'role': 'assistant', 'content': response})
            # ìŒì„±ìœ¼ë¡œ ë³€í™˜
            audio_tag = openai_api.tts(response)
            st.html(audio_tag) # ì‹œê°í™” ì—†ì´ ìë™ ì¬ìƒ

    with col2:
        st.subheader('ì§ˆë¬¸/ë‹µë³€ ğŸ’­')
        if (audio.duration_seconds > 0) and (st.session_state['check_reset'] == False):
            for i, message in enumerate(st.session_state['messages']):
                role = message['role']
                content = message['content']
                if role == 'user': # ì‚¬ìš©ì
                    msg(content, is_user = True, key=str(i))
                elif role == 'assistant': # ai ë‹µë³€
                    msg(content, is_user = False, key=str(i))
        else:
            # ì´ˆê¸°í™” ë²„íŠ¼ ëˆ„ë¥´ë©´, í™”ë©´ì´ ì •ë¦¬ë˜ê³ , ë‹¤ì‹œ check_resetì„ ì›ìƒë³µêµ¬
            st.session_state['check_reset'] = False



if __name__ == '__main__':
    main()